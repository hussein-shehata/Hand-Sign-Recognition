{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m saved_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSaved_Model_With_TF1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39msave(model, saved_model_path)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mMobileNetWithoutTheLast6Layers.h5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaved Model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "saved_model_path = \"Saved_Model_With_TF1\"\n",
    "tf.saved_model.save(model, saved_model_path)\n",
    "model.save(\"MobileNetWithoutTheLast6Layers.h5\")\n",
    "print(\"Saved Model\")\n",
    "saved_model_path = \"Saved_Model_With_TFWith_Keras\"\n",
    "tf.keras.models.save_model(model, saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(\"0_Saved_Model_With_TF1\")\n",
    "loaded_model_keras = tf.keras.models.load_model(\"0_Saved_Model_With_TFWith_Keras\")\n",
    "loaded_model_keras_h5 = tf.keras.models.load_model('0_MobileNetWithoutTheLast6Layers.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "loaded_Pruned_model_keras_h5 = tf.keras.models.load_model('MobileNetWithoutTheLast9Layers.h5', custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_keras_h5.save_weights(\"PretrainedWeights.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataSet from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "pixels = 224\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 16#@param {type:\"integer\"}\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "\n",
    "def build_dataset(subset):\n",
    "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=.20,\n",
    "      subset=subset,\n",
    "      label_mode=\"categorical\",\n",
    "      # Seed needs to provided when using validation_split and shuffle = True.\n",
    "      # A fixed seed is used so that the validation set is stable across runs.\n",
    "      seed=123,\n",
    "      image_size=IMAGE_SIZE,\n",
    "      batch_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "msh fahm hna leh b3ml kol dah \n",
    "fahm 7tt el normalization layer bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87000 files belonging to 29 classes.\n",
      "Using 69600 files for training.\n",
      "Found 87000 files belonging to 29 classes.\n",
      "Using 17400 files for validation.\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "\n",
    "train_ds = build_dataset(\"training\")\n",
    "class_names = tuple(train_ds.class_names)\n",
    "train_size = train_ds.cardinality().numpy()\n",
    "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "train_ds = train_ds.repeat()\n",
    "\n",
    "train_ds = train_ds.map(lambda images, labels:\n",
    "                        (preprocessing_model(images), labels))\n",
    "\n",
    "val_ds = build_dataset(\"validation\")\n",
    "valid_size = val_ds.cardinality().numpy()\n",
    "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "val_ds = val_ds.map(lambda images, labels:\n",
    "                    (normalization_layer(images), labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088/1088 [==============================] - 384s 351ms/step - loss: 0.0992 - accuracy: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0992448478937149, 0.9798850417137146]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_Pruned_model_keras_h5.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "loaded_Pruned_model_keras_h5.evaluate(val_ds,verbose = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tare2a tania 3shan a3ml load l el dataset \n",
    "\n",
    "bst5dm m3aha .fit_generator msh .fit bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87000 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_data_dir = 'D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m (\u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(train_generator)\u001b[39m/\u001b[39mbatch_size)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)): \u001b[39m#1st batch is already fetched before the for loop.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m   img, label \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(train_generator)\n\u001b[1;32m----> 8\u001b[0m   X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mappend(X_train, img, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m )\n\u001b[0;32m      9\u001b[0m   y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(y_train, label, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(X_train\u001b[39m.\u001b[39mshape, y_train\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Scorpio\\miniconda3\\envs\\GPEnv\\lib\\site-packages\\numpy\\lib\\function_base.py:5499\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5497\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[0;32m   5498\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 5499\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "train_generator.reset()\n",
    "batch_size = 1\n",
    "X_train, y_train = next(train_generator)\n",
    "for i in (range(int(len(train_generator)/batch_size)-1)): #1st batch is already fetched before the for loop.\n",
    "  img, label = next(train_generator)\n",
    "  X_train = np.append(X_train, img, axis=0 )\n",
    "  y_train = np.append(y_train, label, axis=0)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tare2a a5od beha el DataSet bardo tania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train\\A\n",
      "D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train\\B\n",
      "D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train\\C\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m label_one_hot_encodded[label_val] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(cpath):\n\u001b[1;32m---> 21\u001b[0m     image_array \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mimread(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(cpath, img), cv2\u001b[39m.\u001b[39;49mIMREAD_COLOR)\n\u001b[0;32m     22\u001b[0m     image_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(image_array)\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m\n\u001b[0;32m     23\u001b[0m     data\u001b[39m.\u001b[39mappend(image_array)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "path = 'D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train'\n",
    "data = []\n",
    "label = []\n",
    "\n",
    "label_one_hot_encodded = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "labels_one_hot_encodded =[]\n",
    "\n",
    "Files = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "label_val = 0\n",
    "for files in Files:\n",
    "    cpath = os.path.join(path, files)\n",
    "    cpath = os.path.join(cpath)\n",
    "    print(cpath)\n",
    "    label_one_hot_encodded[label_val] = 1\n",
    "    for img in os.listdir(cpath):\n",
    "        image_array = cv2.imread(os.path.join(cpath, img), cv2.IMREAD_COLOR)\n",
    "        image_array = cv2.resize(image_array,(224,224))\n",
    "        # image_array = np.array(image_array)/255\n",
    "        data.append(image_array)\n",
    "        label.append(label_val)\n",
    "\n",
    "        labels_one_hot_encodded.append(label_one_hot_encodded.copy()) \n",
    "\n",
    "\n",
    "    label_one_hot_encodded[label_val] = 0 #reseting to the old vector\n",
    "    label_val = label_val + 1\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# data = np.asarray(data)\n",
    "# label = np.asarray(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 649ms/step\n",
      "[[1.3639607e-10 6.9615353e-09 9.9999785e-01 2.6756791e-11 3.1700759e-10\n",
      "  2.6224194e-12 3.5286693e-12 2.2860066e-08 2.1222627e-08 4.7620571e-12\n",
      "  2.5463021e-12 1.0656387e-06 1.0537037e-10 2.2493651e-07 1.3452154e-08\n",
      "  1.7005860e-08 5.8649471e-07 1.1057926e-12 1.7534789e-07 7.6919872e-13\n",
      "  5.7855819e-15 2.1180832e-10 1.1783766e-09 1.0313684e-13 2.6977274e-09\n",
      "  5.1682449e-09 2.7027306e-11 2.9993477e-10 2.5444285e-08]]\n",
      "Predicted label: C\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('MobileNetWithoutTheLast6Layers.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "import cv2\n",
    "classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "word2 = cv2.imread(\"test_forC.jpeg\")\n",
    "word2 = cv2.resize(word2,(224,224))     # resize image to match model's expected sizing\n",
    "\n",
    "\n",
    "def preprocess_Input(X):\n",
    "    np_X = np.array(X)\n",
    "    normalised_X = np_X.astype('float32')/255.0\n",
    "    return normalised_X\n",
    "\n",
    "word2 = preprocess_Input(word2)\n",
    "\n",
    "\n",
    "\n",
    "prediction_scores = model.predict(np.expand_dims(word2, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(prediction_scores)\n",
    "print(\"Predicted label: \" + classes[predicted_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted label: A\n",
      "Actual label: A\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted label: B\n",
      "Actual label: B\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted label: C\n",
      "Actual label: C\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted label: D\n",
      "Actual label: D\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Predicted label: E\n",
      "Actual label: E\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Predicted label: F\n",
      "Actual label: F\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Predicted label: G\n",
      "Actual label: G\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted label: H\n",
      "Actual label: H\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Predicted label: I\n",
      "Actual label: I\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Predicted label: J\n",
      "Actual label: J\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Predicted label: K\n",
      "Actual label: K\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicted label: L\n",
      "Actual label: L\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted label: M\n",
      "Actual label: M\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted label: N\n",
      "Actual label: N\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Predicted label: O\n",
      "Actual label: O\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted label: P\n",
      "Actual label: P\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "Predicted label: Q\n",
      "Actual label: Q\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "Predicted label: R\n",
      "Actual label: R\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "Predicted label: S\n",
      "Actual label: S\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Predicted label: T\n",
      "Actual label: T\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Predicted label: U\n",
      "Actual label: U\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Predicted label: V\n",
      "Actual label: V\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predicted label: W\n",
      "Actual label: W\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "Predicted label: X\n",
      "Actual label: X\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted label: Y\n",
      "Actual label: Y\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted label: Z\n",
      "Actual label: Z\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Predicted label: del\n",
      "Actual label: del\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "Predicted label: nothing\n",
      "Actual label: nothing\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Predicted label: space\n",
      "Actual label: space\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "path = 'Test Images'\n",
    "# model = tf.keras.models.load_model('words_model_kaggle.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "for i in range(0,29) :\n",
    "    currentImage = classes[i]+\"_test.jpg\"\n",
    "    currentImage = os.path.join(path, currentImage)\n",
    "    word2 = cv2.imread(currentImage)\n",
    "    # word2 = cv2.resize(word2,(200,200))     # resize image to match model's expected sizing\n",
    "\n",
    "    def preprocess_Input(X):\n",
    "        np_X = np.array(X)\n",
    "        normalised_X = np_X.astype('float32')/255.0\n",
    "        return normalised_X\n",
    "\n",
    "    word2 = preprocess_Input(word2)\n",
    "    prediction_scores = loaded_Pruned_model_keras_h5.predict(np.expand_dims(word2, axis=0))\n",
    "    predicted_index = np.argmax(prediction_scores)\n",
    "    print(\"Predicted label: \" + classes[predicted_index])\n",
    "    print(\"Actual label: \" + classes[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cacc230392650fa4286313c795167cdc364f7039153613550d3da1c389fd26be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
