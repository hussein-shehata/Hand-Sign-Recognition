{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"Saved_Model_With_TF1\"\n",
    "tf.saved_model.save(model, saved_model_path)\n",
    "model.save(\"MobileNetWithoutTheLast6Layers.h5\")\n",
    "print(\"Saved Model\")\n",
    "saved_model_path = \"Saved_Model_With_TFWith_Keras\"\n",
    "tf.keras.models.save_model(model, saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(\"0_Saved_Model_With_TF1\")\n",
    "loaded_model_keras = tf.keras.models.load_model(\"0_Saved_Model_With_TFWith_Keras\")\n",
    "loaded_model_keras_h5 = tf.keras.models.load_model('0_MobileNetWithoutTheLast6Layers.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "loaded_Pruned_model_keras_h5 = tf.keras.models.load_model('MobileNetWithoutTheLast9Layers.h5', custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_keras_h5.save_weights(\"PretrainedWeights.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataSet from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "pixels = 224\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 16#@param {type:\"integer\"}\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "\n",
    "def build_dataset(subset):\n",
    "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=.20,\n",
    "      subset=subset,\n",
    "      label_mode=\"categorical\",\n",
    "      # Seed needs to provided when using validation_split and shuffle = True.\n",
    "      # A fixed seed is used so that the validation set is stable across runs.\n",
    "      seed=123,\n",
    "      image_size=IMAGE_SIZE,\n",
    "      batch_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "msh fahm hna leh b3ml kol dah \n",
    "fahm 7tt el normalization layer bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "\n",
    "train_ds = build_dataset(\"training\")\n",
    "class_names = tuple(train_ds.class_names)\n",
    "train_size = train_ds.cardinality().numpy()\n",
    "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "train_ds = train_ds.repeat()\n",
    "\n",
    "train_ds = train_ds.map(lambda images, labels:\n",
    "                        (preprocessing_model(images), labels))\n",
    "\n",
    "val_ds = build_dataset(\"validation\")\n",
    "valid_size = val_ds.cardinality().numpy()\n",
    "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "val_ds = val_ds.map(lambda images, labels:\n",
    "                    (normalization_layer(images), labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_Pruned_model_keras_h5.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "loaded_Pruned_model_keras_h5.evaluate(val_ds,verbose = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tare2a tania 3shan a3ml load l el dataset \n",
    "\n",
    "bst5dm m3aha .fit_generator msh .fit bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_data_dir = 'D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "train_generator.reset()\n",
    "batch_size = 1\n",
    "X_train, y_train = next(train_generator)\n",
    "for i in (range(int(len(train_generator)/batch_size)-1)): #1st batch is already fetched before the for loop.\n",
    "  img, label = next(train_generator)\n",
    "  X_train = np.append(X_train, img, axis=0 )\n",
    "  y_train = np.append(y_train, label, axis=0)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tare2a a5od beha el DataSet bardo tania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "path = 'D:/kolya/4th year/GP/New Idea/GitHub Codes/Sign-Language-Translator-main/ASL Alphabitic/asl_alphabet_train/asl_alphabet_train'\n",
    "data = []\n",
    "label = []\n",
    "\n",
    "label_one_hot_encodded = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "labels_one_hot_encodded =[]\n",
    "\n",
    "Files = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "label_val = 0\n",
    "for files in Files:\n",
    "    cpath = os.path.join(path, files)\n",
    "    cpath = os.path.join(cpath)\n",
    "    print(cpath)\n",
    "    label_one_hot_encodded[label_val] = 1\n",
    "    for img in os.listdir(cpath):\n",
    "        image_array = cv2.imread(os.path.join(cpath, img), cv2.IMREAD_COLOR)\n",
    "        image_array = cv2.resize(image_array,(224,224))\n",
    "        # image_array = np.array(image_array)/255\n",
    "        data.append(image_array)\n",
    "        label.append(label_val)\n",
    "\n",
    "        labels_one_hot_encodded.append(label_one_hot_encodded.copy()) \n",
    "\n",
    "\n",
    "    label_one_hot_encodded[label_val] = 0 #reseting to the old vector\n",
    "    label_val = label_val + 1\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# data = np.asarray(data)\n",
    "# label = np.asarray(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('MobileNetWithoutTheLast6Layers.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "import cv2\n",
    "classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "word2 = cv2.imread(\"test_forC.jpeg\")\n",
    "word2 = cv2.resize(word2,(224,224))     # resize image to match model's expected sizing\n",
    "\n",
    "\n",
    "def preprocess_Input(X):\n",
    "    np_X = np.array(X)\n",
    "    normalised_X = np_X.astype('float32')/255.0\n",
    "    return normalised_X\n",
    "\n",
    "word2 = preprocess_Input(word2)\n",
    "\n",
    "\n",
    "\n",
    "prediction_scores = model.predict(np.expand_dims(word2, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(prediction_scores)\n",
    "print(\"Predicted label: \" + classes[predicted_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "path = 'Test Images'\n",
    "# model = tf.keras.models.load_model('words_model_kaggle.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "for i in range(0,29) :\n",
    "    currentImage = classes[i]+\"_test.jpg\"\n",
    "    currentImage = os.path.join(path, currentImage)\n",
    "    word2 = cv2.imread(currentImage)\n",
    "    # word2 = cv2.resize(word2,(200,200))     # resize image to match model's expected sizing\n",
    "\n",
    "    def preprocess_Input(X):\n",
    "        np_X = np.array(X)\n",
    "        normalised_X = np_X.astype('float32')/255.0\n",
    "        return normalised_X\n",
    "\n",
    "    word2 = preprocess_Input(word2)\n",
    "    prediction_scores = loaded_Pruned_model_keras_h5.predict(np.expand_dims(word2, axis=0))\n",
    "    predicted_index = np.argmax(prediction_scores)\n",
    "    print(\"Predicted label: \" + classes[predicted_index])\n",
    "    print(\"Actual label: \" + classes[i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convert Squeeze Net trained model in pytorch to a keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "print(onnx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import keras\n",
    "import torchvision\n",
    "from pytorch2keras.converter import pytorch_to_keras\n",
    "\n",
    "# Load the saved SqueezeNet model\n",
    "squeezenet = nn.Sequential()\n",
    "squeezenet.features = torch.nn.Sequential(*(list(torchvision.models.squeezenet1_1(pretrained=True).features.children())))\n",
    "squeezenet.classifier = torch.nn.Sequential(*(list(torchvision.models.squeezenet1_1(pretrained=True).classifier.children())))\n",
    "\n",
    "# Convert the model to Keras\n",
    "input_shape = (3, 224, 224)  # channels first\n",
    "keras_model = pytorch_to_keras(squeezenet, input_var=torch.zeros((1, *input_shape)))\n",
    "\n",
    "# Save the Keras model to a file\n",
    "keras_model.save('squeezenet.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnx2keras\n",
    "\n",
    "# Load the pre-trained PyTorch model\n",
    "model = torch.hub.load('pytorch/vision:v0.9.0', 'squeezenet1_0', pretrained=True)\n",
    "\n",
    "# Export the PyTorch model to ONNX format\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "onnx_model_path = \"squeezenet.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "# Convert the ONNX model to Keras format\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "k_model = onnx2keras.onnx_to_keras(onnx_model,['input.1'])\n",
    "\n",
    "# Save the Keras model to disk\n",
    "k_model.save(\"squeezenet.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:onnx2keras:Converter is called.\n",
      "DEBUG:onnx2keras:List input shapes:\n",
      "DEBUG:onnx2keras:None\n",
      "DEBUG:onnx2keras:List inputs:\n",
      "DEBUG:onnx2keras:Input 0 -> input.1.\n",
      "DEBUG:onnx2keras:List outputs:\n",
      "DEBUG:onnx2keras:Output 0 -> 117.\n",
      "DEBUG:onnx2keras:Gathering weights to dictionary.\n",
      "DEBUG:onnx2keras:Found weight features.0.weight with shape (96, 3, 7, 7).\n",
      "DEBUG:onnx2keras:Found weight features.0.bias with shape (96,).\n",
      "DEBUG:onnx2keras:Found weight features.3.squeeze.weight with shape (16, 96, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.3.squeeze.bias with shape (16,).\n",
      "DEBUG:onnx2keras:Found weight features.3.expand1x1.weight with shape (64, 16, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.3.expand1x1.bias with shape (64,).\n",
      "DEBUG:onnx2keras:Found weight features.3.expand3x3.weight with shape (64, 16, 3, 3).\n",
      "DEBUG:onnx2keras:Found weight features.3.expand3x3.bias with shape (64,).\n",
      "DEBUG:onnx2keras:Found weight features.4.squeeze.weight with shape (16, 128, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.4.squeeze.bias with shape (16,).\n",
      "DEBUG:onnx2keras:Found weight features.4.expand1x1.weight with shape (64, 16, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.4.expand1x1.bias with shape (64,).\n",
      "DEBUG:onnx2keras:Found weight features.4.expand3x3.weight with shape (64, 16, 3, 3).\n",
      "DEBUG:onnx2keras:Found weight features.4.expand3x3.bias with shape (64,).\n",
      "DEBUG:onnx2keras:Found weight features.5.squeeze.weight with shape (32, 128, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.5.squeeze.bias with shape (32,).\n",
      "DEBUG:onnx2keras:Found weight features.5.expand1x1.weight with shape (128, 32, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.5.expand1x1.bias with shape (128,).\n",
      "DEBUG:onnx2keras:Found weight features.5.expand3x3.weight with shape (128, 32, 3, 3).\n",
      "DEBUG:onnx2keras:Found weight features.5.expand3x3.bias with shape (128,).\n",
      "DEBUG:onnx2keras:Found weight features.7.squeeze.weight with shape (32, 256, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.7.squeeze.bias with shape (32,).\n",
      "DEBUG:onnx2keras:Found weight features.7.expand1x1.weight with shape (128, 32, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.7.expand1x1.bias with shape (128,).\n",
      "DEBUG:onnx2keras:Found weight features.7.expand3x3.weight with shape (128, 32, 3, 3).\n",
      "DEBUG:onnx2keras:Found weight features.7.expand3x3.bias with shape (128,).\n",
      "DEBUG:onnx2keras:Found weight features.8.squeeze.weight with shape (48, 256, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.8.squeeze.bias with shape (48,).\n",
      "DEBUG:onnx2keras:Found weight features.8.expand1x1.weight with shape (192, 48, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.8.expand1x1.bias with shape (192,).\n",
      "DEBUG:onnx2keras:Found weight features.8.expand3x3.weight with shape (192, 48, 3, 3).\n",
      "DEBUG:onnx2keras:Found weight features.8.expand3x3.bias with shape (192,).\n",
      "DEBUG:onnx2keras:Found weight features.9.squeeze.weight with shape (48, 384, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.9.squeeze.bias with shape (48,).\n",
      "DEBUG:onnx2keras:Found weight features.9.expand1x1.weight with shape (192, 48, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.9.expand1x1.bias with shape (192,).\n",
      "DEBUG:onnx2keras:Found weight features.9.expand3x3.weight with shape (192, 48, 3, 3).\n",
      "DEBUG:onnx2keras:Found weight features.9.expand3x3.bias with shape (192,).\n",
      "DEBUG:onnx2keras:Found weight features.10.squeeze.weight with shape (64, 384, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.10.squeeze.bias with shape (64,).\n",
      "DEBUG:onnx2keras:Found weight features.10.expand1x1.weight with shape (256, 64, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.10.expand1x1.bias with shape (256,).\n",
      "DEBUG:onnx2keras:Found weight features.10.expand3x3.weight with shape (256, 64, 3, 3).\n",
      "DEBUG:onnx2keras:Found weight features.10.expand3x3.bias with shape (256,).\n",
      "DEBUG:onnx2keras:Found weight features.12.squeeze.weight with shape (64, 512, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.12.squeeze.bias with shape (64,).\n",
      "DEBUG:onnx2keras:Found weight features.12.expand1x1.weight with shape (256, 64, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight features.12.expand1x1.bias with shape (256,).\n",
      "DEBUG:onnx2keras:Found weight features.12.expand3x3.weight with shape (256, 64, 3, 3).\n",
      "DEBUG:onnx2keras:Found weight features.12.expand3x3.bias with shape (256,).\n",
      "DEBUG:onnx2keras:Found weight classifier.1.weight with shape (1000, 512, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight classifier.1.bias with shape (1000,).\n",
      "DEBUG:onnx2keras:Found input input.1 with shape [3, 224, 224]\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Conv\n",
      "DEBUG:onnx2keras:node_name: /features/features.0/Conv_output_0\n",
      "DEBUG:onnx2keras:node_params: {'dilations': [1, 1], 'group': 1, 'kernel_shape': [7, 7], 'pads': [0, 0, 0, 0], 'strides': [2, 2], 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name input.1).\n",
      "DEBUG:onnx2keras:Check input 1 (name features.0.weight).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name features.0.bias).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:conv:Conv with bias\n",
      "DEBUG:onnx2keras:conv:2D convolution\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'/features/features.0/Conv_output_0/' is not a valid root scope name. A root scope name has to match the following pattern: ^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m onnx_model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39msqueezenet.onnx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Call the converter (input - is the main model input name, can be different for your model)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m k_model \u001b[39m=\u001b[39m onnx_to_keras(onnx_model, [\u001b[39m'\u001b[39;49m\u001b[39minput.1\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\Scorpio\\miniconda3\\envs\\torch2keras\\lib\\site-packages\\onnx2keras\\converter.py:175\u001b[0m, in \u001b[0;36monnx_to_keras\u001b[1;34m(onnx_model, input_names, input_shapes, name_policy, verbose, change_ordering)\u001b[0m\n\u001b[0;32m    172\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39m... found all, continue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    174\u001b[0m keras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mset_image_data_format(\u001b[39m'\u001b[39m\u001b[39mchannels_first\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 175\u001b[0m AVAILABLE_CONVERTERS[node_type](\n\u001b[0;32m    176\u001b[0m     node,\n\u001b[0;32m    177\u001b[0m     node_params,\n\u001b[0;32m    178\u001b[0m     layers,\n\u001b[0;32m    179\u001b[0m     lambda_funcs,\n\u001b[0;32m    180\u001b[0m     node_name,\n\u001b[0;32m    181\u001b[0m     keras_names\n\u001b[0;32m    182\u001b[0m )\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(keras_names, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    184\u001b[0m     keras_names \u001b[39m=\u001b[39m keras_names[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Scorpio\\miniconda3\\envs\\torch2keras\\lib\\site-packages\\onnx2keras\\convolution_layers.py:177\u001b[0m, in \u001b[0;36mconvert_conv\u001b[1;34m(node, params, layers, lambda_func, node_name, keras_name)\u001b[0m\n\u001b[0;32m    162\u001b[0m             weights \u001b[39m=\u001b[39m [W]\n\u001b[0;32m    164\u001b[0m         conv \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\n\u001b[0;32m    165\u001b[0m             filters\u001b[39m=\u001b[39mout_channels,\n\u001b[0;32m    166\u001b[0m             kernel_size\u001b[39m=\u001b[39m(height, width),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m             name\u001b[39m=\u001b[39mkeras_name\n\u001b[0;32m    175\u001b[0m         )\n\u001b[1;32m--> 177\u001b[0m         layers[node_name] \u001b[39m=\u001b[39m conv(input_0)\n\u001b[0;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[39m# 1D conv\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     W \u001b[39m=\u001b[39m W\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Scorpio\\miniconda3\\envs\\torch2keras\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Scorpio\\miniconda3\\envs\\torch2keras\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen)\n\u001b[0;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: '/features/features.0/Conv_output_0/' is not a valid root scope name. A root scope name has to match the following pattern: ^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx2keras import onnx_to_keras\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model = onnx.load('squeezenet.onnx')\n",
    "\n",
    "# Call the converter (input - is the main model input name, can be different for your model)\n",
    "k_model = onnx_to_keras(onnx_model, ['input.1'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "onnx_model = onnx.load(\"squeezenet.onnx\")  # load onnx model\n",
    "tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "tf_rep.export_graph(\"output_path\")  # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_keras = tf.keras.models.load_model(\"Saved Models For SqueezeNet/0_Saved_Model_With_TF1\")\n",
    "# loaded_model_keras = tf.keras.models.load_model(\"Saved Models For MobileNet/4-Saved_Model_With_TFWith_Keras_without_the_last_9_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"Saved_Model_With_TF1\"\n",
    "tf.saved_model.save(loaded_model_keras, saved_model_path)\n",
    "loaded_model_keras.save(\"MobileNetWithoutTheLast6Layers.h5\")\n",
    "print(\"Saved Model\")\n",
    "saved_model_path = \"Saved_Model_With_TFWith_Keras\"\n",
    "tf.keras.models.save_model(loaded_model_keras, saved_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59ac60012996f9a2c991151ad4872081427e8551c666b49aab9c0dde18607692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
